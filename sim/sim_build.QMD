---
title: "baesRCM Simulation Build"
author: "Quinton Neville"
date: "10/25/2022"
format: 
  html:
    toc: true
    toc-depth: 2
  pdf:
    toc: true
    toc-depth: 2
header-includes:
   \usepackage{float}
   \floatplacement{figure}{H}
---

```{r include = FALSE, error = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(gt)
library(MASS)
library(Rcpp)
library(RcppArmadillo)
library(glasso)
library(GIGrvg)
#library(doParallel)
library(Matrix)

#Controlling figure output in markdown, setting options & root dir
knitr::opts_chunk$set(
#  fig.height =   
  fig.width = 6,
#  fig.asp = .5,
  out.width = "90%",
#  out.height = 
 fig.align = "center",
  cache = FALSE,
  echo  = TRUE,
  root.dir = rprojroot::find_package_root_file() #not working?
)

#My Colours (from viridis)
my_purple <- "#440154FF"
my_yellow <- "#FDE725FF"
#Set Theme for ggplot2
theme_set(theme_minimal() + 
          theme(plot.title = element_text(hjust = 0.5),
                plot.subtitle = element_text(hjust = 0.5),
                legend.position = "bottom"))
#Set Scientific notation output for knitr
options(scipen = 999)
```

# 1. Generate Some Simple Data  

Here we generate 10 volumes of multivariate normal data for 10 subjects in a network of 4 rois, with 2 true connections or edges in the associated group graph.  

```{r sim_data, warning = FALSE}
#Generate covariance structure for multivariate gaussian covariance matrices 
volumes  <- 500
subjects <- 20
rois     <- 10 #keep it small to start testing 
true_con <- 20 #true connections or no. of edges in G

#Start with G_0, Omega_0 (Sigma_0^{-1})
#A. Group overall graph G_0with true_con # of edges
##1. Start with diagonal p x p matrix
G_0 <- diag(TRUE, subjects)

##2. To induce relative sparsity, make every 3rd off diagonal an edge
G_0[upper.tri(G_0)][seq(1, (subjects * (subjects - 1) / 2), by = 3)] <- TRUE

##3. Enforce symmetry by the upper trial
G_0         <- G_0 + t(G_0) - diag(1, subjects) #Subtract extra 1 from diagonal
G_0_logical <- (G_0 == 1) #Make logical for below

##.4 Generate Omega_0 from G_0 (via Lin's simu_data.R schema)
#Sample
set.seed(4)
sample <- runif(subjects^2, 0.5, 1)
set.seed(4)
sample <- sample * sample(c(-1, 1), subjects^2, replace = TRUE)

#Threshold by G_0
init     <- diag(0.5, subjects) + G_0 * matrix(sample, nrow = subjects, ncol = subjects)
init_sym <- init + t(init) + diag(rowSums(G_0 + t(G_0))/2) #Ensure symmetry & p.d. X + t(X) + diag()

#True Omega_0
Omega_0 <- diag(diag(init_sym)^-0.5) %*% init_sym %*% diag(diag(init_sym)^-0.5)
Sigma_0 <- solve(Omega_0)

#Check positive definite
if (any(eigen(Omega_0, only.values = TRUE)$values <= 0) | any(eigen(Sigma_0, only.values = TRUE)$values <= 0)) { 
  #Warning
  warning("Warning: one of Omega_0/Sigma_0 not p.d.")
}

##B. G_k, Omega_k ~ GWish(G_k, tau_k, Omega_0/tau_k)
#Set up storage and params
G_k      <- list()
Omega_k  <- list()
Sigma_k  <- list()
Tau_k    <- vector(mode = "numeric", length = subjects)
n_edges  <- 1 #Off diagonal elements to flip/change for each subject
lambda_2 <- 1/50 #Look at larger values of tau_k i.e. smaller lambda2 

#Loop through subjects
for (k in 1:subjects) {
  #Set seed, sample an off diagonal element / edge to flip
  set.seed(k)
  new_edges <- sample(1:(rois * (rois - 1) / 2), n_edges)
  
  #1. (G_k) Randomly add or remove n_edge # of edges in upper.tri(G_0) to generate
  G_k[[k]] <- G_0_logical
  G_k[[k]][upper.tri(G_k[[k]], diag = FALSE)][new_edges] <- !G_k[[k]][upper.tri(G_k[[k]], diag = FALSE)][new_edges] #Flip those upper tri edges
  G_k[[k]] <- as.matrix(Matrix::forceSymmetric(G_k[[k]], uplo = "U"))
  
  #Check to make sure working appropriately
  if (sum(G_k[[k]] != G_0) != 2 * n_edges) {
    warning("woops! Something's wrong, not flipping the desired number of edges.")
  }
  
  #2. (Tau_k) Randomly sim from Exp(\lambda_2 = 2) [rgwish only takes b = tau_k > 2 => 2 + Exp(lambda_2)?]
  set.seed(k)
  Tau_k[k] <- rexp(1, lambda_2) #Shifted exponential ? that seems better
  
  #3. (Omega_k) ~ GWish(G_k, Tau_k, Omega_0/Tau_k) => Omega_k^{-1} = Sigma_0 for mvtnorm
  tri_adj <- G_k[[k]]
  tri_adj[lower.tri(tri_adj, diag = TRUE)] <- FALSE #Only upper tri adj for rgwish
  
  #Set seed & sample from RGwish
  set.seed(k)
  Omega_k[[k]] <- round(BDgraph::rgwish(1, adj = tri_adj, b = Tau_k[k] + 2, D = Sigma_0 * Tau_k[k]), 4)
  Sigma_k[[k]] <- solve(Omega_k[[k]])
}

##C. (Y_ki) Data itself (data_list of length k subjects)
#Create data list 
data_list <- list()

#Loop through subjects and volumes to generate data
for (k in 1:subjects) { #assumes no temporal mean trend, centered at 0
    set.seed(k)
    data_list[[k]] <- mvtnorm::rmvnorm(volumes, rep(0, subjects), Sigma_k[[k]])
}

#Summary of difference between Omega_k and Omega_0 by subject
#Raw difference in values
summary(map_dbl(.x = Omega_k, ~mean(.x - Omega_0)))
summary(map_dbl(.x = Omega_k, ~mean(solve(.x) - Sigma_0)))

#Difference in zeros (should be no more than 2 * n_edge in max/min direction)
summary(map_dbl(.x = Omega_k, ~mean(sum(abs(.x[upper.tri(.x, diag = FALSE)]) <= 0.001) - sum(Omega_0[upper.tri(Omega_0, diag = FALSE)] == 0))))
```


# 2. Deconstruct main `rcm()` call from `bayesRCM` package functions for debugging    

In development, one can source the helper functions from the `./R/` directory and the `.cpp` file from `/src/`. However, at this moment the package is functional and so one can download via github with `devtools::install_github("nevilleq/bayesRCM")` and then load the library like normal `library(bayesRCM)`.  

```{r src, echo = FALSE, results = "hide", include = FALSE}
# #Source the necessary data reading, generating, and aipw model from ./src
# source_folder <- "./R/"
# source_files  <- list.files(source_folder, pattern = ".R")
# 
# #Iteratively source
# map(
#   .x = source_files, 
#   ~source(str_c(source_folder, .x)) %>%
#    invisible()
# )
# 
# #For cpp
# source_cpps <- list.files(source_folder, pattern = ".cpp")
# sourceCpp(file = str_c(source_folder, source_cpps))
library(bayesRCM)

#Function to attach date to file_name
date_file <- function(file_name) {
  #String combine todays date with the desired filename
  str_c(
    Sys.Date() %>%
      str_replace_all("-",  "_"),
    file_name,
    sep = "_"
  ) %>%
    return() #return a string w date_filename
}
```

## 2.1 Posterior Samples (Prelim Test) 

Next, let's generate some preliminary posterior samples for 10 iterations and look at the results. There are still bugs we need to work out, especially in the graph update `log_H` and being able to do a cholesky decomp (i.e. encforcing symmetry and positive definite-ness).  

```{r eval = FALSE}
result <- rcm(y = data_list, n_samples = 10)
write_rds(result, "./results/prelim_test.RDS")
```



## 2.2 Testing Individual Updates by Parameter  

Here, we fix all MCMC, graph updates, and/or non-direct sampling components of the algorithm, then observe the behaviour of the resulting Markov Chain(s). To do so, we are going to pull out the `rcm` source code, fix the desired elements at the "truth" (see above), and then sample the parameter(s) of interest. This should help troubleshoot and debug, especially for the $G_k/\Omega_k$ update.  

### 2.2.1 $\Omega_0$ with fixed $\tau_k$    

```{r omega_0, eval = FALSE, echo = FALSE}
#Set params
y <- data_list
n_samples <- 5000
n_burn <- 500
n_updates <- 5
    
  #Grab no. of subjects, rois, volumes
  K  <- length(y)
  p  <- ncol(y[[1]])
  vk <- map_dbl(y, nrow)
  Sk <- map(.x = y, ~t(.x) %*% .x)

  #Set lambda 1-3 penalty gamma a, b  hyperparams
  alpha <- c(0.5, 1, 0.5) #1 - G_k, 2 - tau_k, 3 - Omega_0 (glasso)
  beta  <- c(0.5, 1, 0.5)

  #Set tau's MH stepsize
  step_tau  <- rep(1, K)
  #n_updates <- 10

  #Initialize estimates for Omega_k, Omega_0
  #Omega_k - tune lambda by bic and then grab G and Omega_0
  lambda_grid <- 10^seq(-3, 0, length.out = 10)
  bic         <- vector(mode = "numeric", length = 0)

  #Tune lambda for independent glasso
  for (lam in lambda_grid) {
    omega_k <- ind_graphs(y, 0.1)
    bic   <- c(bic, bic_cal(y, omega_k))
  }
  #Compute initial est. via best bic
  omega_k <- ind_graphs(y, lambda_grid[which.min(bic)]) #Overestimates Omega_0, seems sensitive to high starting value
  
  #Loop through just to make sure PD
  for (k in 1:K) {
    if (any(eigen(omega_k[[k]])$values < 0)) {
      warning(str_c("omega_", k, "was not positive definite in sim."))
      omega_k[[k]] <- 
        omega_k[[k]] |>
        (\(x) {as.matrix(Matrix::nearPD(x)$mat)})()
    }
  }
  
  #Note here, for small tau_k (0.1-ish), individual omega_k get large --> over estimated Omega_0 to start
  #Maybe a better scheme for initializing Omega_0? Or implementing a similar adaptive window/step size?
  adj_k   <- map(.x = omega_k, ~abs(.x) > 0.001)
  #Omega0  <- apply(abind::abind(omega_k, along=3),1:2,mean)
  omega_0 <- Reduce("+", omega_k) / K

  #Set up storage for results
  #Omegas
  omegas_res <- array(NA, c(p * (p + 1) / 2, K, n_samples))
  omega0_res <- array(NA, c(p * (p + 1) / 2, n_samples))
  pct_omega_acc <- vector(mode = "integer", length = n_samples)
  pct_k_acc  <- matrix(NA, nrow = n_samples, ncol = K)

  #Taus
  accept_mat    <- matrix(NA, nrow = 0, ncol = K)
  step_tau_mat  <- step_tau #Adaptive window for MH tau
  tau_res       <- array(NA, c(K, n_samples))

  #Lambdas
  lambda_res <- array(NA, c(3, n_samples), dimnames = list(str_c("lambda_", 1:3)))

  #Set timer
  timer   <- 0
  t_start <- proc.time()
  n_iter  <- (n_burn + n_samples)

  #Loop through sampling algorithm n_samples + n_burn # times
  for (t in 1:n_iter) {
    #Print iteration for early testing
    #print(paste0("Iteration: ", t))
    
    #Set fixed tau, omega_k at truth from above
    omega_k <- Omega_k
    tau_vec <- Tau_k
    
    #Update Lambdas via direct sampling
    #Lambda 1 sparsity-inducing penalty on G_k
    #card_k   <- (sapply(adj_k, sum) - p)/2 #Cardinality of G_k / # edges
    #lambda_1 <- rgamma(1, alpha[1] + K, rate = beta[1] + sum(card_k))

    #Lambda 2 Exponential rate parameter for df/shrinkage tau_k prior
    #lambda_2 <- rgamma(1, alpha[2] + K + 1, beta[2] + sum(tau_vec))

    #Lambda 3 Sparse L-1 penalty on group precision omega_0 prior
    #From Zhang2014 Sparse Covariance Decomp/Sampling
    card_0 <- (sum(abs(omega_0) > 0.001) + p) / 2 #Cardinality/# Edges or non-zeros
    lambda_3 <- rgamma(1, alpha[3] + card_0, beta[3] + sum(abs(omega_0))/2)

    #Update Omega_0 via Wang and Li (2012) + step-proposal distribution
    D          <- apply(mapply('*', omega_k, tau_vec, SIMPLIFY = 'array'), 1:2, sum)
    omega_0    <- omega0_update(omega_0, D, sum(tau_vec), lambda_3)
    pct_accept <- omega_0$pct_accept #Off-diagonal acceptance%
    omega_0    <- omega_0$omega #Precision matrix itself

    #Save those results after burn-in
    if(t > n_burn) {
      t_burn <- t - n_burn
      #omegas_res[, , t_burn] <- sapply(omega_k, function(x) x[upper.tri(x, diag = TRUE)])
      omega0_res[, t_burn]   <- omega_0[upper.tri(omega_0, diag = TRUE)]
      #tau_res[, t_burn]      <- tau_k
      lambda_res[, t_burn]   <- c(NA, NA, lambda_3)
      pct_omega_acc[t_burn]  <- pct_accept
      #pct_k_acc[t_burn, ]    <- accept_k
    }

    #Track temporal progress (every 20% progress update)
    if (t %% floor(0.2 * (n_samples + n_burn))) {
      t_now   <- proc.time()
      timer   <- c(timer, (t_now - t_start)[3])
      t_start <- t_now
    }
  }

  #Result list of results
  omega0_result <-
    list(
      omega_0     = omega0_res,
      omega_acc   = pct_omega_acc,
      lambda3_res = lambda_res,
      timer       = timer
    )
  
  #Write out for safekeeping
  write_rds(omega0_result, str_c("../results/", date_file("debug_omega0.RDS")))
  
  #Accepting everything at the MH step, is that good? seems wrong 
```

```{r omega0_diag, echo = FALSE}
#Read in result to display diagnostics
result <- read_rds("../results/2023_01_30_debug_omega0.RDS")
n_iter <- ncol(result$omega_0)
set.seed(4)
samp  <- sample(1:nrow(result$omega_0), 25, replace = FALSE)

#Thin the chain
thin_result <- result$omega_0[,seq(1, n_iter, by = 2)]
n_iter <- ncol(thin_result)

#Diagnostics
chain0.gg <- 
#  result$omega_0 %>%
  thin_result %>%
  as.data.frame() %>%
  slice(samp) %>%
  rename_with(
    .cols = everything(),
    ~str_remove(.x, "V")
  ) %>%
  mutate(
    upper_tri = 1:nrow(.)
  ) %>%
  pivot_longer(
    cols = -upper_tri,
    names_to = "iteration",
    values_to = "value"
  ) %>%
  as_tibble() %>%
  ggplot(aes(x = as.numeric(iteration), y = value, colour = upper_tri, group = upper_tri)) +
  geom_point(size = 0.8, alpha = 0.6) +
  geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~upper_tri, scales = "free_y", ncol = 5) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_colour_viridis_c() +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Omega_0 Chain with Fixed tau_k and G_k/Omega_k"
  ) +
  scale_x_continuous(breaks = seq(0, n_iter, by = floor(1/4*n_iter))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 0.5))

#Posterior density GG
#Diagnostics
post_density.gg <- 
#  result$omega_0 %>%
  thin_result %>%
  as.data.frame() %>%
  slice(samp) %>%
  rename_with(
    .cols = everything(),
    ~str_remove(.x, "V")
  ) %>%
  mutate(
    upper_tri = 1:nrow(.) %>% as.factor()
  ) %>%
  pivot_longer(
    cols = -upper_tri,
    names_to = "iteration",
    values_to = "value"
  ) %>%
  mutate(
    upper_tri = as.factor(upper_tri) %>% fct_reorder(value, mean, .desc = TRUE)
  ) %>%
  ggplot(aes(x = value, y = ..density.., fill = upper_tri, colour = upper_tri)) +
  geom_histogram(alpha = 0.4) +
  geom_density(alpha = 0.6, colour = "black") + 
  #geom_point(size = 0.8, alpha = 0.6) +
  #geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~upper_tri, scales = "free", ncol = 5) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_colour_viridis_d(option = "plasma") +
  scale_fill_viridis_d(option = "plasma") +
  labs(
    x = "MCMC Sampled Value",
    y = "Density",
    title = "Omega_0 Density with Fixed tau_k and G_k/Omega_k"
  ) #+
  #stat_summary(aes(y = value), fun = mean, geom = "point", size = 6, shape = "|") 

#Acceptance
accept0.gg <- 
  tibble(
    x = result$omega_acc
  ) %>%
  ggplot(aes(x = x)) +
  geom_density(colour = "black", fill = my_purple, alpha = 0.2) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = "Percent Off-Diagonal Acceptance (MH-step)",
    y = "Density",
    title = "Omega_0 Off-diagonal Acceptance Rate"
  )
```

```{r omega0_diag_display, echo = FALSE, fig.height=10, fig.width=6 }
#Display Chain
chain0.gg

#Dispaly
post_density.gg # %>% ggsave("./funny_plot.png", .)
```

```{r omega0_acceptance, echo = FALSE}
#Display acceptance off off diagonal elements in MH step
#accept0.gg
```

```{r post_est_diag}
##Precision/Omega
#Take posterior mean/mode as estimates
post_mean  <- apply(result$omega_0, 1, mean)   #L2
post_med   <- apply(result$omega_0, 1, median) #L1

#Lambda 2 Post mean
result$lambda3_res %>% 
  apply(., 1, mean) -> lam_2_post_est

#post_mode  <- apply(result$omega_0, 1, stat_mode) # Need a user defined mode func if so
omega_true <- Omega_0[upper.tri(Omega_0, diag = TRUE)]

#Mean absolute error - raw difference
map(.x = list(post_mean, post_med), ~summary(.x - omega_true))
map(.x = list(post_mean, post_med), ~max(abs(.x - omega_true)))

##Variance/Sigma
post_omega0 <- Sigma_0 #Just for correct size, going to overwrite
post_omega0[upper.tri(post_omega0, diag = TRUE)]  <- post_mean
post_omega0 <- as.matrix(Matrix::forceSymmetric(post_omega0, uplo = "U"))

post_sigma0 <- solve(post_omega0) #Sigma posterior est.
sigma0_est  <- post_sigma0[upper.tri(post_sigma0, diag = TRUE)]
sigma_true  <- Sigma_0[upper.tri(Sigma_0, diag = TRUE)]

#Summarise difference/error
summary(sigma0_est - sigma_true)

#Summarise difference in norms (L1, Inf, Frobenius, Spectral) - comparison 
norm_types <- c("1", "F", "2")
post_sigma0_norms <- map_dbl(.x = norm_types, ~norm(post_sigma0, type = .x))
post_omega0_norms <- map_dbl(.x = norm_types, ~norm(post_omega0, type = .x))
sigma_true_norms  <- map_dbl(.x = norm_types, ~norm(Sigma_0, type = .x))
omega_true_norms  <- map_dbl(.x = norm_types, ~norm(Omega_0, type = .x))

#Display Norm results
tibble(
  `Norm Type`   = str_c(c("L1", "Frobenius", "Spectral"), " Norm"),
  `Sigma Est.`  = post_sigma0_norms,
  `Sigma Truth` = sigma_true_norms,
  `Sigma Diff.` = map_dbl(.x = norm_types, ~norm(post_sigma0 - Sigma_0, type = .x)),
  `Omega Est.`  = post_omega0_norms,
  `Omega Truth` = omega_true_norms,
  `Omega Diff.` = map_dbl(.x = norm_types, ~norm(post_omega0 - Omega_0, type = .x))
) %>%
  group_by(`Norm Type`) %>%
  gt() 
```

### 2.2.2 $\tau_k$ with fixed $\Omega_0, \Omega_k/G_k$  

```{r tauk, eval = TRUE, echo = FALSE}
#Set sampling params
y <- data_list
n_samples <- 100
n_burn <- 0
n_updates <- 0

  #Set lambda 1-3 penalty gamma a, b  hyperparams
  alpha <- c(0.5, 1, 0.5) #1 - G_k, 2 - tau_k, 3 - Omega_0 (glasso)
  beta  <- c(0.5, 1, 0.5)

#Sample sizes
  K  <- length(y)
  p  <- ncol(y[[1]])

#Initialize Omega_k estimates for initial values
  #Omega_k - tune lambda by bic and then grab G and Omega_0
  lambda_grid <- 10^seq(-3, 0, length.out = 10)
  bic         <- vector(mode = "numeric", length = 0)

  #Tune lambda for independent glasso
  for (lam in lambda_grid) {
    omega_k <- ind_graphs(y, 0.1)
    bic   <- c(bic, bic_cal(y, omega_k))
  }
  #Compute initial est. via best bic
  omega_k <- ind_graphs(y, lambda_grid[which.min(bic)])
  adj_k   <- map(.x = omega_k, ~abs(.x) > 0.001)
  #Omega0  <- apply(abind::abind(omega_k, along=3),1:2,mean)
  omega_0 <- Reduce("+", omega_k) / K
  
  #Loop through just to make sure PD
  for (k in 1:K) {
    if (any(eigen(omega_k[[k]])$values < 0)) {
      warning(str_c("omega_", k, "was not positive definite in sim."))
      omega_k[[k]] <- 
        omega_k[[k]] |>
        (\(x) {as.matrix(Matrix::nearPD(x)$mat)})()
    }
  }
  
  #Initialize estimates for tau_k
  #Tau vector of subject specific regularization param on Omega_0
  tau_vec <- vector(mode = "numeric", length = K)
  #Set tau's MH stepsize
  step_tau  <- rep(1, K)

  #Iterate over each subject, find optimal tau_k based on posterior
  for (k in 1:K) {
    #print(k)
    #Tau posterior for fixed omega_k, omega_0, and lambda_2 = 0
    f_opt <- function(tau) {
      -1 * log_tau_posterior(tau, Omega_k[[k]], omega_0, lambda_2 = 0)
    }
    #Optimize in 1D
    tau_vec[k] <- c(optimize(f_opt, lower = 1, upper = 100, tol = 0.01)$min)
  }

  #Set up storage for results
  #Omegas
  #omegas_res <- array(NA, c(p * (p + 1) / 2, K, n_samples))
  #omega0_res <- array(NA, c(p * (p + 1) / 2, n_samples))
  #pct_omega_acc <- vector(mode = "integer", length = n_samples)
  #pct_k_acc  <- matrix(NA, nrow = n_samples, ncol = K)

  #Taus
  accept_mat    <- matrix(NA, nrow = 0, ncol = K)
  step_tau_mat  <- step_tau #Adaptive window for MH tau
  tau_res       <- array(NA, c(K, n_samples))

  #Lambdas
  lambda_res <- array(NA, c(3, n_samples), dimnames = list(str_c("lambda_", 1:3)))

  #Set timer
  timer   <- 0
  t_start <- proc.time()
  n_iter  <- (n_burn + n_samples)
  #n_iter  <- 3

  #Loop through sampling algorithm n_samples + n_burn # times
  for (t in 1:n_iter) {
    #Print iteration for early testing
    #print(paste0("Iteration: ", t))
    
    #Update Lambdas via direct sampling
    #Lambda 1 sparsity-inducing penalty on G_k
    #card_k   <- (sapply(adj_k, sum) - p)/2 #Cardinality of G_k / # edges
    #lambda_1 <- rgamma(1, alpha[1] + K, rate = beta[1] + sum(card_k))

    #Lambda 2 Exponential rate parameter for df/shrinkage tau_k prior
    lambda_2 <- rgamma(1, alpha[2] + K + 1, beta[2] + sum(tau_vec))

    #Lambda 3 Sparse L-1 penalty on group precision omega_0 prior
    #card_0 <- (sum(abs(omega_0) > 0.001) + p) / 2 #Cardinality omega_0 / # Edges
    #lambda_3 <- rgamma(1, alpha[3] + card_0, beta[3] + norm(omega_0, type = "1"))
    
    #Update G_k, Omega_k via modified BIPS proposal & update scheme - Wang and Li (2012)
    omega_k <- Omega_k
    omega_0 <- Omega_0

    #Tau_k update
    tau_k <-
      map(
        .x = 1:K, #Iterate from index 1 to K
        ~tau_update(tau_vec[.x], omega_k[[.x]], omega_0, lambda_2, step_tau[.x])
      ) #Return list object
    accept_mat <- rbind(accept_mat, tau_k %>% map_lgl("accept")) #Pull out acceptance
    tau_vec    <- tau_k %>% map_dbl("tau_k") #Pull out the numeric tau_k list object
    
    #Adaptive tau window/stepsize ~ variance/sigma in log normal
     if (t %% n_updates == 0 & t <= n_burn) {
        #Compute acceptance rate (colwise mean)
        accept_rate <- apply(accept_mat, 2, mean)
        #For each subject, adjust tau_k proposal (lognormal) step size
         for (k in 1:K) {
           if (accept_rate[k] > 0.75) { #If accepting to many, inc variance of proposal
            step_tau[k] <- step_tau[k] + 0.05
           } else if (accept_rate[k] < 0.5) { #If not accepting enough, dec variance of proposal
             step_tau[k] <- max(0.05, step_tau[k] - 0.05)
            }
         }
         step_tau_mat  <- rbind(step_tau_mat, step_tau) #Record adaptive step sizes
         accept_mat    <- matrix(NA, nrow = 0, ncol = K) #Restart acceptance rate tracking
     }

    #Save those results after burn-in
    if(t > n_burn) {
      t_burn <- t - n_burn
      #omegas_res[, , t_burn] <- sapply(omega_k, function(x) x[upper.tri(x, diag = TRUE)])
      #omega0_res[, t_burn]   <- omega_0[upper.tri(omega_0, diag = TRUE)]
      tau_res[, t_burn]      <- tau_vec
      lambda_res[, t_burn]   <- c(NA, lambda_2, NA)
      #pct_omega_acc[t_burn]  <- pct_accept
      #pct_k_acc[t_burn, ]    <- accept_k
    }

    #Track temporal progress (every 20% progress update)
    if (t %% floor(0.2 * (n_samples + n_burn))) {
      t_now   <- proc.time()
      timer   <- c(timer, (t_now - t_start)[3])
      t_start <- t_now
    }
  }

  #Result list of results
  tau_k_result <-
    list(
      tau_k     = tau_res,
      lambda_2  = lambda_res[2, ], 
      tau_acc   = accept_mat,
      tau_step  = step_tau_mat
    )
  
  #Write out for safekeeping
  write_rds(tau_k_result,  str_c("../results/", date_file("debug_tauk.RDS")))
```

```{r tauk_diag, echo = FALSE}
#Read in result to display diagnostics
result <- read_rds("../results/2023_01_30_debug_tauk.RDS")
n_iter <- ncol(result$tau_k)
set.seed(4)

#Diagnostics
#Tauk
tauk.gg <-
  t(result$tau_k) %>%
  as.data.frame() %>%
  mutate(
    iteration = 1:nrow(.)
  ) %>%
  rename_with(
    .cols = -iteration,
    ~str_replace(.x, "V", "Sub. ")
  ) %>%
  pivot_longer(
    cols = -iteration,
    names_to = "tau",
    values_to = "value"
  ) %>%
  mutate(
    tau = as.factor(tau) %>% fct_reorder(value, mean, .desc = TRUE)
  ) %>%
  ggplot(aes(x = iteration, y = value, colour = tau)) +
  geom_point(size = 0.8, alpha = 0.6) +
  geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~tau, scales = "free_y", nrow = 5) +
  scale_x_continuous(breaks = seq(0, n_iter, by = floor(1/4*n_iter)), 
                     minor_breaks = seq(0, n_iter, by = floor(1/4*n_iter))) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_colour_viridis_d() +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Tau_k Chain with Fixed G_k/Omega_k and Omega_0"
  )

#Tau_k acceptance
tauk_acc.gg <-
result$tau_acc %>%
  as.data.frame() %>%
  mutate(
    iteration = 1:nrow(.)
  ) %>%
  rename_with(
    .cols = -iteration,
    ~str_replace(.x, "V", "Sub. ")
  ) %>%
  pivot_longer(
    cols = -iteration,
    names_to = "tau",
    values_to = "value"
  ) %>%
  group_by(tau) %>%
  summarise(pct_acc = mean(value)) %>%
  ungroup() %>%
  mutate(
    tau = as.factor(tau) %>% fct_reorder(pct_acc, .desc = FALSE)
  ) %>%
  arrange(tau) %>%
  ggplot(aes(x = tau, y = pct_acc, colour = pct_acc, fill = pct_acc)) +
  geom_col() +
  scale_colour_viridis_c("Pct. Accept", breaks = seq(0.9, 1, by = 0.05), labels = scales::percent, direction = -1) +
  scale_fill_viridis_c("Pct. Accept", breaks = seq(0.9, 1, by = 0.05), labels = scales::percent, direction = -1) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Subject",
    y = "Tau Acceptance",
    title = "Tau Acceptance for Fixed G_k/Omega_k and Omega_0"
  ) +
  theme(legend.position = "left") +
  coord_flip() +
  theme_minimal()
```

```{r tauk_diag_display, echo = FALSE, fig.height=6, fig.width=6}
tauk.gg
tauk_acc.gg
result$tau_step %>%
  as.data.frame() %>%
  mutate(tau_step = paste0("Adaptive step ", 1:nrow(.))) %>%
  rename_with(
    .cols = everything(),
    ~str_replace(.x, "V", "Subj. ")
  ) %>%
  dplyr::select(tau_step, everything()) %>%
  group_by(tau_step) %>%
  gt() %>%
  tab_header(title = "Adaptive Tau-proposal Step-size/Variance")
```


### 2.2.3 $\Omega_0$ with free $\tau_k$, fixed $G_k/\Omega_k$

```{r omega_0_tauk, eval = FALSE, echo = FALSE}
  #Grab no. of subjects, rois, volumes
  K  <- length(y)
  p  <- ncol(y[[1]])
  vk <- map_dbl(y, nrow)
  Sk <- map(.x = y, ~t(.x) %*% .x)

  #Set lambda 1-3 penalty gamma a, b  hyperparams
  alpha <- c(0.5, 1, 0.5) #1 - G_k, 2 - tau_k, 3 - Omega_0 (glasso)
  beta  <- c(0.5, 1, 0.5)

  #Set tau's MH stepsize
  step_tau  <- rep(1, K)
  #n_updates <- 5

  #Initialize estimates for Omega_k, Omega_0
  #Omega_k - tune lambda by bic and then grab G and Omega_0
  lambda_grid <- 10^seq(-3, 0, length.out = 10)
  bic         <- vector(mode = "numeric", length = 0)

  #Tune lambda for independent glasso
  for (lam in lambda_grid) {
    omega_k <- ind_graphs(y, 0.1)
    bic   <- c(bic, bic_cal(y, omega_k))
  }
  #Compute initial est. via best bic
  omega_k <- ind_graphs(y, lambda_grid[which.min(bic)])
  
  #Loop through just to make sure PD
  for (k in 1:K) {
    if (any(eigen(omega_k[[k]])$values < 0)) {
      #print(k)
      omega_k[[k]] <- 
        omega_k[[k]] |>
        (\(x) {as.matrix(Matrix::nearPD(x)$mat)})()
    }
  }
  
  adj_k   <- map(.x = omega_k, ~abs(.x) > 0.001)
  #Omega0  <- apply(abind::abind(omega_k, along=3),1:2,mean)
  omega_0 <- Reduce("+", omega_k) / K

  #Initialize estimates for tau_k
  #Tau vector of subject specific regularization param on Omega_0
  tau_vec <- vector(mode = "numeric", length = K)

  #Iterate over each subject, find optimal tau_k based on posterior
  for (k in 1:K) {
    #print(k)
    #Tau posterior for fixed omega_k, omega_0, and lambda_2 = 0
    f_opt <- function(tau) {
      -1 * log_tau_posterior(tau, omega_k[[k]], omega_0, lambda_2 = 0)
    }
    #Optimize in 1D
    tau_vec[k] <- c(optimize(f_opt, interval = c(0, 1000), tol = 10)$min)
  }

  #Set up storage for results
  #Omegas

  #Taus
  accept_mat    <- matrix(NA, nrow = 0, ncol = K)
  step_tau_mat  <- step_tau #Adaptive window for MH tau
  tau_res       <- array(NA, c(K, n_samples))

  #Lambdas
  lambda_res <- array(NA, c(3, n_samples), dimnames = list(str_c("lambda_", 1:3)))

  #Set timer
  timer   <- 0
  t_start <- proc.time()
  n_iter  <- (n_burn + n_samples)
  #n_iter  <- 3

  #Loop through sampling algorithm n_samples + n_burn # times
  for (t in 1:n_iter) {
    #Print iteration for early testing
    #print(paste0("Iteration: ", t))
    
    #Update Lambdas via direct sampling
    #Lambda 1 sparsity-inducing penalty on G_k
    card_k   <- (sapply(adj_k, sum) - p)/2 #Cardinality of G_k / # edges
    lambda_1 <- rgamma(1, alpha[1] + K, rate = beta[1] + sum(card_k))

    #Lambda 2 Exponential rate parameter for df/shrinkage tau_k prior
    lambda_2 <- rgamma(1, alpha[2] + K + 1, beta[2] + sum(tau_vec))

    #Lambda 3 Sparse L-1 penalty on group precision omega_0 prior
    card_0 <- (sum(abs(omega_0) > 0.001) + p) / 2 #Cardinality omega_0 / # Edges
    lambda_3 <- rgamma(1, alpha[3] + card_0, beta[3] + norm(omega_0, type = "1"))

    #Invert for Covariance & randomly select row_col pair
    sigma_0 <- matinv(omega_0)
    row_col <- sample(1:p, 1)
    
    #Set up foreach:: combine into 2 list, multicombine = TRUE
    # my_combine <- function(x, ...) {
    #   lapply(seq_along(x),
    #          function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
    # }
    
    #Update G_k, Omega_k via modified BIPS proposal & update scheme - Wang and Li (2012)
    omega_k <- omega_k

    #Tau_k update
    tau_k <-
      map(
        .x = 1:K, #Iterate from index 1 to K
        ~tau_update(tau_vec[.x], omega_k[[.x]], omega_0, lambda_2, step_tau[.x])
      ) #Return list object
    accept_mat <- rbind(accept_mat, tau_k %>% map_lgl("accept")) #Pull out acceptance
    tau_vec    <- tau_k %>% map_dbl("tau_k") #Pull out the numeric tau_k list object
    
    #Adaptive tau window/stepsize ~ variance/sigma in log normal
     if (t %% n_updates == 0 & t <= n_burn) {
        #Compute acceptance rate (colwise mean)
        accept_rate <- apply(accept_mat, 2, mean)
        #For each subject, adjust tau_k proposal (lognormal) step size
         for (k in 1:K) {
           if (accept_rate[k] > 0.75) { #If accepting to many, inc variance of proposal
            step_tau[k] <- step_tau[k] + 0.05
           } else if (accept_rate[k] < 0.5) { #If not accepting enough, dec variance of proposal
             step_tau[k] <- max(0.05, step_tau[k] - 0.05)
            }
         }
         step_tau_mat  <- rbind(step_tau_mat, step_tau) #Record adaptive step sizes
         accept_mat    <- matrix(NA, nrow = 0, ncol = K) #Restart acceptance rate tracking
     }

    #Update Omega_0 via Wang and Li (2012) + step-proposal distribution
    D       <- apply(mapply('/', omega_k, tau_vec, SIMPLIFY = 'array'), 1:2, sum)
    omega_0 <- omega0_update(omega_0, D, sum(tau_vec), lambda_3)
    pct_accept <- omega_0$pct_accept #Off-diagonal acceptance%
    omega_0 <- omega_0$omega #Precision matrix itself

    #Save those results after burn-in
    if(t > n_burn) {
      t_burn <- t - n_burn
      #omegas_res[, , t_burn] <- sapply(omega_k, function(x) x[upper.tri(x, diag = TRUE)])
      omega0_res[, t_burn]   <- omega_0[upper.tri(omega_0, diag = TRUE)]
      tau_res[, t_burn]      <- tau_vec
      #lambda_res[, t_burn]   <- c(lambda_1, lambda_2, lambda_3)
      pct_omega_acc[t_burn]  <- pct_accept
      #pct_k_acc[t_burn, ]    <- accept_k
    }

    #Track temporal progress (every 20% progress update)
    if (t %% floor(0.2 * (n_samples + n_burn))) {
      t_now   <- proc.time()
      timer   <- c(timer, (t_now - t_start)[3])
      t_start <- t_now
    }
  }

  #Result list of results
  omega0_result <-
    list(
      omega_0   = omega0_res,
      tau_k     = tau_res,
      tau_acc   = accept_mat,
      omega_acc = pct_omega_acc,
      timer     = timer
    )
  
  #Write out for safekeeping
  write_rds(omega0_result, "../results/prelim_omega0_tauk.RDS")
```

```{r omega0_tauk_diag, eval = FALSE, echo = FALSE}
#Read in result to display diagnostics
result <- read_rds("../results/prelim_omega0_tauk.RDS")
n_iter <- ncol(result$omega_0)
set.seed(4)
samp  <- sample(1:nrow(result$omega_0), 25, replace = FALSE)

#Diagnostics
chain0.gg <- 
  result$omega_0 %>%
  as.data.frame() %>%
  slice(samp) %>%
  rename_with(
    .cols = everything(),
    ~str_remove(.x, "V")
  ) %>%
  mutate(
    upper_tri = 1:nrow(.)
  ) %>%
  pivot_longer(
    cols = -upper_tri,
    names_to = "iteration",
    values_to = "value"
  ) %>%
  ggplot(aes(x = as.numeric(iteration), y = value, colour = upper_tri, group = upper_tri)) +
  geom_point(size = 0.8, alpha = 0.6) +
  geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~upper_tri, scales = "free_y", nrow = 5) +
  scale_x_continuous(breaks = seq(0, n_iter, by = floor(1/4*n_iter)), 
                     minor_breaks = seq(0, n_iter, by = floor(1/4*n_iter))) +
  scale_y_continuous(labels = scales::scientific) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_colour_viridis_c() +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Omega_0 Chain with Fixed G_k/Omega_k and Free Tau_k"
  )

#Acceptance
accept0.gg <- 
  tibble(
    x = result$omega_acc
  ) %>%
  ggplot(aes(x = x)) +
  geom_density(colour = "black", fill = my_purple, alpha = 0.2) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = "Percent Off-Diagonal Acceptance (MH-step)",
    y = "Density",
    title = "Omega_0 Off-diagonal Acceptance Rate"
  )

#Tauk
tauk.gg <-
  t(result$tau_k) %>%
  as.data.frame() %>%
  mutate(
    iteration = 1:nrow(.)
  ) %>%
  rename_with(
    .cols = -iteration,
    ~str_replace(.x, "V", "Sub. ")
  ) %>%
  pivot_longer(
    cols = -iteration,
    names_to = "tau",
    values_to = "value"
  ) %>%
  mutate(
    tau = as.factor(tau) %>% fct_reorder(value, mean, .desc = TRUE)
  ) %>%
  ggplot(aes(x = iteration, y = value, colour = tau)) +
  geom_point(size = 0.8, alpha = 0.6) +
  geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~tau, scales = "free_y", nrow = 5) +
  scale_x_continuous(breaks = seq(0, n_iter, by = floor(1/4*n_iter)), 
                     minor_breaks = seq(0, n_iter, by = floor(1/4*n_iter))) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Tau_k Chain with Fixed G_k/Omega_k, Varied Omega_0"
  )

#Tau_k acceptance
tauk_acc.gg <-
result$tau_acc %>%
  as.data.frame() %>%
  mutate(
    iteration = 1:nrow(.)
  ) %>%
  rename_with(
    .cols = -iteration,
    ~str_replace(.x, "V", "Sub. ")
  ) %>%
  pivot_longer(
    cols = -iteration,
    names_to = "tau",
    values_to = "value"
  ) %>%
  group_by(tau) %>%
  summarise(pct_acc = mean(value)) %>%
  ungroup() %>%
  mutate(
    tau = as.factor(tau) %>% fct_reorder(pct_acc, .desc = FALSE)
  ) %>%
  arrange(tau) %>%
  ggplot(aes(x = tau, y = pct_acc, colour = pct_acc, fill = pct_acc)) +
  geom_col() +
  scale_colour_viridis_c("Pct. Accept", breaks = seq(0.9, 1, by = 0.05), labels = scales::percent, direction = -1) +
  scale_fill_viridis_c("Pct. Accept", breaks = seq(0.9, 1, by = 0.05), labels = scales::percent, direction = -1) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Subject",
    y = "Tau Acceptance",
    title = "Tau Acceptance for Fixed G_k/Omega_k but Varied Omega_0"
  ) +
  theme(legend.position = "left") +
  coord_flip() +
  theme_minimal()
```

```{r omega0_tauk_diag_display, echo = FALSE, fig.height = 6, fig.width = 6}
chain0.gg
accept0.gg
tauk.gg
tauk_acc.gg
```

### 2.2.4 $\Omega_k$ & $G_k$ with fixed $\Omega_0$ and $\tau_k$

```{r g_omega_k, eval = FALSE, echo = FALSE}
n_samples <- 100
n_burn <- 0
n_updates <- 0
#cl <- parallel::makeCluster(n_cores)
#  on.exit(parallel::stopCluster(cl)) #When done stop cluster
#  doParallel::registerDoParallel(cl) #Initialize clusters
#  `%dopar%` <- foreach::`%dopar%` #Need to pass this function later
    
  #Grab no. of subjects, rois, volumes
  K  <- length(y)
  p  <- ncol(y[[1]])
  vk <- map_dbl(y, nrow)
  Sk <- map(.x = y, ~t(.x) %*% .x)

  #Set lambda 1-3 penalty gamma a, b  hyperparams
  alpha <- c(0.5, 1, 0.5) #1 - G_k, 2 - tau_k, 3 - Omega_0 (glasso)
  beta  <- c(0.5, 1, 0.5)
  
#Initialize Omega_k estimates for initial values
  #Omega_k - tune lambda by bic and then grab G and Omega_0
  lambda_grid <- 10^seq(-3, 0.5, length.out = 20)
  bic         <- vector(mode = "numeric", length = 0)

  #Tune lambda for independent glasso
  for (lam in lambda_grid) {
    omega_k <- ind_graphs(y, 0.1)
    bic   <- c(bic, bic_cal(y, omega_k))
  }
  #Compute initial est. via best bic
  omega_k <- ind_graphs(y, lambda_grid[which.min(bic)])
  adj_k   <- map(.x = omega_k, ~abs(.x) > 0.01)
  #omega_0 <- Reduce("+", omega_k) / K

  #Set up storage for results
  #Omegas
  omegas_res <- array(NA, c(p * (p + 1) / 2, K, n_samples))
  #omega0_res <- array(NA, c(p * (p + 1) / 2, n_samples))
  #pct_omega_acc <- vector(mode = "integer", length = n_samples)
  pct_k_acc  <- matrix(NA, nrow = n_samples, ncol = K)

  #Lambdas
  lambda_res <- array(NA, c(3, n_samples), dimnames = list(str_c("lambda_", 1:3)))

  #Set timer
  timer   <- 0
  t_start <- proc.time()
  n_iter  <- (n_burn + n_samples)
  #n_iter  <- 3

  #Loop through sampling algorithm n_samples + n_burn # times
  for (t in 1:n_iter) {
    #Print iteration for early testing
    print(paste0("Iteration: ", t))
    
    #Update Lambdas via direct sampling
    #Lambda 1 sparsity-inducing penalty on G_k
    card_k   <- (sapply(adj_k, sum) - p)/2 #Cardinality of G_k / # edges
    lambda_1 <- rgamma(1, alpha[1] + K, rate = beta[1] + sum(card_k))

    #Lambda 2 Exponential rate parameter for df/shrinkage tau_k prior
    #lambda_2 <- rgamma(1, alpha[2] + K + 1, beta[2] + sum(tau_vec))

    #Lambda 3 Sparse L-1 penalty on group precision omega_0 prior
    #card_0 <- (sum(abs(omega_0) > 0.001) + p) / 2 #Cardinality omega_0 / # Edges or non-zero elements
    #lambda_3 <- rgamma(1, alpha[3] + card_0, beta[3] + sum(abs(omega_0))/2)

    #Invert for Covariance & randomly select row_col pair
    sigma_0 <- matinv(Omega_0)
    tau_vec <- Tau_k
    row_col <- sample(1:p, 1)
    
    #Set up foreach:: combine into 2 list, multicombine = TRUE
    # my_combine <- function(x, ...) {
    #   lapply(seq_along(x),
    #          function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
    # }
    
#     #Update G_k, Omega_k via modified BIPS proposal & update scheme - Wang and Li (2012)
#     omega_k <- foreach::foreach(
#         k         = 1:K, #Subject index
# #        .combine  = "c", #List output
# #        .multicombine = TRUE,
# #        .init     = list(omega_k = list(), accept = list()),
#         .packages = c("bayesRCM", "BDgraph", "Matrix"), #Packages
#         .noexport   = c("graph_update","gwish_ij_update", "rgwish", "isSymmetric", "forceSymmetric") #Functions necessary to export
#       ) %dopar% {
        #Initialize Acceptance  
        accept_k <- vector(mode = "numeric", length = K)
        #print(paste0("Subject -- ", k))
        omega_k_update <- list()
        for(k in 1:k) {
        print(paste0("Subject -- ", k))
        #Propose/update G_k, Omega_k via
        update <-
          graph_update(
            row_col  = row_col,
            df       = tau_vec[k] + 2,
            D        = sigma_0 * tau_vec[k],
            v        = vk[k],
            S        = Sk[[k]],
            adj      = adj_k[[k]],
            omega    = omega_k[[k]],
            lambda_1 = lambda_1
          )
        
        #Record acceptance rate
        accept_k[k] <- update$accept

        #Upper triangular and averaged transpose for computational stability
        tri_adj <- update$adj
        tri_adj[lower.tri(tri_adj, diag = T)] <- 0
  
        #Update omega_k
        omega_k_update[[k]] <- BDgraph::rgwish(1, tri_adj, vk[k] + tau_vec[k] + 2, Sk[[k]] + sigma_0 * tau_vec[k])
        omega_k_update[[k]] <- (omega_k_update[[k]] + t(omega_k_update[[k]])) / 2 # for computational stability

        #Return updated omega_k
        #omega_k_update
        #return(omega_k_update)
        }
        
      omega_k <- omega_k_update

    #Update Omega_0 via Wang and Li (2012) + step-proposal distribution
    #omega_0 <- Omega_0 #Precision matrix itself

    #Save those results after burn-in
    if(t > n_burn) {
      t_burn <- t - n_burn
      omegas_res[, , t_burn] <- sapply(omega_k, function(x) x[upper.tri(x, diag = TRUE)])
      #omega0_res[, t_burn]   <- omega_0[upper.tri(omega_0, diag = TRUE)]
      #tau_res[, t_burn]      <- tau_k
      lambda_res[, t_burn]   <- c(lambda_1)
      #pct_omega_acc[t_burn]  <- pct_accept
      pct_k_acc[t_burn, ]    <- accept_k
    }

    #Track temporal progress (every 20% progress update)
    if (t %% floor(0.2 * (n_samples + n_burn))) {
      t_now   <- proc.time()
      timer   <- c(timer, (t_now - t_start)[3])
      t_start <- t_now
    }
  }

  #Result list of results
  result <-
    list(
      #omega_0   = omega0_res,
      omega_k   = omegas_res,
      accept_k  = pct_k_acc,
      #tau_k     = tau_res,
      lambdas   = lambda_res,
      #tau_acc   = accept_mat,
      #omega_acc = pct_omega_acc,
      #tau_step  = step_tau_mat,
      timer     = timer
    )

```


