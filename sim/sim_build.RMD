---
title: "baesRCM Simulation Build"
author: "Quinton Neville"
date: "9/15/2022"
output:
  pdf_document: 
    df_print: paged
    toc: true
    toc_depth: 2
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
header-includes: \usepackage{graphicx} \usepackage{float} \usepackage{amsmath}
---

```{r include = FALSE, error = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(gt)
library(MASS)
library(Rcpp)
library(RcppArmadillo)
library(glasso)
library(GIGrvg)
#library(doParallel)
library(Matrix)

#Controlling figure output in markdown, setting options & root dir
knitr::opts_chunk$set(
#  fig.height =   
  fig.width = 6,
#  fig.asp = .5,
  out.width = "90%",
#  out.height = 
 fig.align = "center",
  cache = FALSE,
  echo  = TRUE,
  root.dir = rprojroot::find_package_root_file() #not working?
)

#My Colours (from viridis)
my_purple <- "#440154FF"
my_yellow <- "#FDE725FF"
#Set Theme for ggplot2
theme_set(theme_minimal() + 
          theme(plot.title = element_text(hjust = 0.5),
                plot.subtitle = element_text(hjust = 0.5),
                legend.position = "bottom"))
#Set Scientific notation output for knitr
options(scipen = 999)

```

# 1. Generate Some Simple Data  

Here we generate 10 volumes of multivariate normal data for 10 subjects in a network of 4 rois, with 2 true connections or edges in the associated group graph.  
```{r sim_data, warning = FALSE}
#Generate covariance structure for multivariate gaussian covariance matrices 
volumes  <- 10
subjects <- 10
rois     <- 10 #keep it small to start testing
true_con <- 20 #true connections or no. of edges in G
#Group overall graph with true_con # of edges
set.seed(4)
G <- 
  matrix(
    rbinom(volumes * subjects, 1, prob = true_con / (volumes * subjects)),
    nrow = volumes
  )
diag(G) <- 1
G <- Matrix::forceSymmetric(G, uplo = "U")
#G

#Overall Precision Matrix
set.seed(4)
Sigma_0 <- 
  (G * matrix(rnorm((volumes * subjects), 0, 10), nrow = volumes)) |>
  Matrix::forceSymmetric(uplo = "U") |>
  (\(x) {as.matrix(Matrix::nearPD(x)$mat)})()
#Sigma_0

#Subject precision matrices based off of group
Sigma_k <- list()

#Fill matrices in list
for (n in 1:subjects) {
  set.seed(n)
  Sigma_k[[n]] <- (Sigma_0 + matrix(rnorm((volumes * subjects), 0.25/n, 1), nrow = volumes)) |>
  Matrix::forceSymmetric(uplo = "U") |>
  (\(x) {as.matrix(Matrix::nearPD(x)$mat)})()
} 

#List of each subject's array in time/by volume
#randomly generated around mean 0 with subject specific precision
#i.e. stead state
data_list <- list()

#Loop through subjects and volumes to generate data
for (n in 1:subjects) { #assumes no temporal mean trend, centered at 0
    set.seed(n)
    data_list[[n]] <- mvtnorm::rmvnorm(volumes, rep(0, rois), Sigma_k[[n]])
}

#Save 'truth' to evaluate recovery
Omega_0 <- solve(Sigma_0)
Omega_k <- map(Sigma_k, solve)
```


# 2. Apply `bayesRCM` package functions to generate posterior samples of the model 

In development, one can source the helper functions from the `./R/` directory and the `.cpp` file from `/src/`. However, at this moment the package is funcitonal and so one can download via github with `devtools::install_github("nevilleq/bayesRCM")` and then load the library like normal `library(bayesRCM)`.  

```{r src, echo = FALSE, results = "hide", include = FALSE}
# #Source the necessary data reading, generating, and aipw model from ./src
# source_folder <- "./R/"
# source_files  <- list.files(source_folder, pattern = ".R")
# 
# #Iteratively source
# map(
#   .x = source_files, 
#   ~source(str_c(source_folder, .x)) %>%
#    invisible()
# )
# 
# #For cpp
# source_cpps <- list.files(source_folder, pattern = ".cpp")
# sourceCpp(file = str_c(source_folder, source_cpps))
library(bayesRCM)
```

## 2.1 Posterior Samples (Prelim Test) 

Next, let's generate some preliminary posterior samples for 10 iterations and look at the results. There are still bugs we need to work out, especially in the graph update `log_H` and being able to do a cholesky decomp (i.e. encforcing symmetry and positive definite-ness).  

```{r eval = FALSE}
result <- rcm(y = data_list, n_samples = 10)
#write_rds(result, "./results/prelim_test.RDS")
write_rds(result, "../results/prelim_test.RDS")
```



## 2.2 Testing Individual Updates by Parameter  

Here, we fix all MCMC, graph updates, and/or non-direct sampling components of the algorithm, then observe the behaviour of the resulting Markov Chain(s). To do so, we are going to pull out the `rcm` source code, fix the desired elements at the "truth" (see above), and then sample the parameter(s) of interest. This should help troubleshoot and debug, especially for the $G_k/\Omega_k$ update.  

### 2.2.1 $\Omega_0$ with fixed $\tau_k$    

```{r omega_0, eval = FALSE, echo = FALSE}
#Set params
y <- data_list
n_samples <- 1000
n_burn <- 100
n_updates <- 10
    
  #Grab no. of subjects, rois, volumes
  K  <- length(y)
  p  <- ncol(y[[1]])
  vk <- map_dbl(y, nrow)
  Sk <- map(.x = y, ~t(.x) %*% .x)

  #Set lambda 1-3 penalty gamma a, b  hyperparams
  alpha <- c(0.5, 1, 0.5) #1 - G_k, 2 - tau_k, 3 - Omega_0 (glasso)
  beta  <- c(0.5, 1, 0.5)

  #Set tau's MH stepsize
  step_tau  <- rep(1, K)
  #n_updates <- 10

  #Initialize estimates for Omega_k, Omega_0
  #Omega_k - tune lambda by bic and then grab G and Omega_0
  lambda_grid <- 10^seq(-3, 0, length.out = 10)
  bic         <- vector(mode = "numeric", length = 0)

  #Tune lambda for independent glasso
  for (lam in lambda_grid) {
    omega_k <- ind_graphs(y, 0.1)
    bic   <- c(bic, bic_cal(y, omega_k))
  }
  #Compute initial est. via best bic
  omega_k <- ind_graphs(y, lambda_grid[which.min(bic)])
  
  #Loop through just to make sure PD
  for (k in 1:K) {
    if (any(eigen(omega_k[[k]])$values < 0)) {
      #print(k)
      omega_k[[k]] <- 
        omega_k[[k]] |>
        (\(x) {as.matrix(Matrix::nearPD(x)$mat)})()
    }
  }
  
  adj_k   <- map(.x = omega_k, ~abs(.x) > 0.001)
  #Omega0  <- apply(abind::abind(omega_k, along=3),1:2,mean)
  omega_0 <- Reduce("+", omega_k) / K

  #Initialize estimates for tau_k
  #Tau vector of subject specific regularization param on Omega_0
  tau_vec <- vector(mode = "numeric", length = K)

  #Iterate over each subject, find optimal tau_k based on posterior
  for (k in 1:K) {
    #print(k)
    #Tau posterior for fixed omega_k, omega_0, and lambda_2 = 0
    f_opt <- function(tau) {
      -1 * log_tau_posterior(tau, omega_k[[k]], omega_0, lambda_2 = 0)
    }
    #Optimize in 1D
    tau_vec[k] <- c(optimize(f_opt, interval = c(0, 1000), tol = 10)$min)
  }

  #Set up storage for results
  #Omegas
  omegas_res <- array(NA, c(p * (p + 1) / 2, K, n_samples))
  omega0_res <- array(NA, c(p * (p + 1) / 2, n_samples))
  pct_omega_acc <- vector(mode = "integer", length = n_samples)
  pct_k_acc  <- matrix(NA, nrow = n_samples, ncol = K)

  #Taus
  accept_mat    <- matrix(NA, nrow = 0, ncol = K)
  step_tau_mat  <- step_tau #Adaptive window for MH tau
  tau_res       <- array(NA, c(K, n_samples))

  #Lambdas
  lambda_res <- array(NA, c(3, n_samples), dimnames = list(str_c("lambda_", 1:3)))

  #Set timer
  timer   <- 0
  t_start <- proc.time()
  n_iter  <- (n_burn + n_samples)
  #n_iter  <- 3

  #Loop through sampling algorithm n_samples + n_burn # times
  for (t in 1:n_iter) {
    #Print iteration for early testing
    print(paste0("Iteration: ", t))
    
    #Update Lambdas via direct sampling
    #Lambda 1 sparsity-inducing penalty on G_k
    card_k   <- (sapply(adj_k, sum) - p)/2 #Cardinality of G_k / # edges
    lambda_1 <- rgamma(1, alpha[1] + K, rate = beta[1] + sum(card_k))

    #Lambda 2 Exponential rate parameter for df/shrinkage tau_k prior
    lambda_2 <- rgamma(1, alpha[2] + K + 1, beta[2] + sum(tau_vec))

    #Lambda 3 Sparse L-1 penalty on group precision omega_0 prior
    card_0 <- (sum(abs(omega_0) > 0.001) + p) / 2 #Cardinality omega_0 / # Edges
    lambda_3 <- rgamma(1, alpha[3] + card_0, beta[3] + norm(omega_0, type = "1"))

    #Invert for Covariance & randomly select row_col pair
    sigma_0 <- matinv(omega_0)
    row_col <- sample(1:p, 1)
    
    #Set up foreach:: combine into 2 list, multicombine = TRUE
    # my_combine <- function(x, ...) {
    #   lapply(seq_along(x),
    #          function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
    # }
    
    #Update G_k, Omega_k via modified BIPS proposal & update scheme - Wang and Li (2012)
    omega_k <- omega_k

    #Tau_k update
    tau_vec <- tau_vec

    #Update Omega_0 via Wang and Li (2012) + step-proposal distribution
    D       <- apply(mapply('/', omega_k, tau_vec, SIMPLIFY = 'array'), 1:2, sum)
    omega_0 <- omega0_update(omega_0, D, sum(tau_vec), lambda_3)
    pct_accept <- omega_0$pct_accept #Off-diagonal acceptance%
    omega_0 <- omega_0$omega #Precision matrix itself

    #Save those results after burn-in
    if(t > n_burn) {
      t_burn <- t - n_burn
      #omegas_res[, , t_burn] <- sapply(omega_k, function(x) x[upper.tri(x, diag = TRUE)])
      omega0_res[, t_burn]   <- omega_0[upper.tri(omega_0, diag = TRUE)]
      #tau_res[, t_burn]      <- tau_k
      #lambda_res[, t_burn]   <- c(lambda_1, lambda_2, lambda_3)
      pct_omega_acc[t_burn]  <- pct_accept
      #pct_k_acc[t_burn, ]    <- accept_k
    }

    #Track temporal progress (every 20% progress update)
    if (t %% floor(0.2 * (n_samples + n_burn))) {
      t_now   <- proc.time()
      timer   <- c(timer, (t_now - t_start)[3])
      t_start <- t_now
    }
  }

  #Result list of results
  omega0_result <-
    list(
      omega_0   = omega0_res,
      omega_acc = pct_omega_acc,
      timer     = timer
    )
  
  #Write out for safekeeping
  write_rds(omega0_result, "../results/prelim_omega0.RDS")
```

```{r omega0_diag, echo = FALSE}
#Read in result to display diagnostics
result <- read_rds("../results/prelim_omega0.RDS")
n_iter <- ncol(result$omega_0)
set.seed(4)
samp  <- sample(1:nrow(result$omega_0), 25, replace = FALSE)

#Diagnostics
chain0.gg <- 
  result$omega_0 %>%
  as.data.frame() %>%
  slice(samp) %>%
  rename_with(
    .cols = everything(),
    ~str_remove(.x, "V")
  ) %>%
  mutate(
    upper_tri = 1:nrow(.)
  ) %>%
  pivot_longer(
    cols = -upper_tri,
    names_to = "iteration",
    values_to = "value"
  ) %>%
  ggplot(aes(x = as.numeric(iteration), y = value, colour = upper_tri, group = upper_tri)) +
  geom_point(size = 0.8, alpha = 0.6) +
  geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~upper_tri, scales = "free_y", nrow = 5) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_colour_viridis_c() +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Omega_0 Chain with Fixed tau_k and G_k/Omega_k"
  ) +
  scale_x_continuous(breaks = seq(0, n_iter, by = floor(1/4*n_iter)), minor_breaks = seq(0, n_iter, by = floor(1/4*n_iter)))


#Acceptance
accept0.gg <- 
  tibble(
    x = result$omega_acc
  ) %>%
  ggplot(aes(x = x)) +
  geom_density(colour = "black", fill = my_purple, alpha = 0.2) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = "Percent Off-Diagonal Acceptance (MH-step)",
    y = "Density",
    title = "Omega_0 Off-diagonal Acceptance Rate"
  )
```

```{r omega0_diag_display, echo = FALSE, fig.height=6, fig.width=6}
chain0.gg
accept0.gg
```

### 2.2.2 $\Omega_0$ with free $\tau_k$  

```{r omega_0_tauk, eval = FALSE, echo = FALSE}
#Set params
y <- data_list
n_samples <- 100
n_burn <- 20
    
  #Grab no. of subjects, rois, volumes
  K  <- length(y)
  p  <- ncol(y[[1]])
  vk <- map_dbl(y, nrow)
  Sk <- map(.x = y, ~t(.x) %*% .x)

  #Set lambda 1-3 penalty gamma a, b  hyperparams
  alpha <- c(0.5, 1, 0.5) #1 - G_k, 2 - tau_k, 3 - Omega_0 (glasso)
  beta  <- c(0.5, 1, 0.5)

  #Set tau's MH stepsize
  step_tau  <- rep(1, K)
  #n_updates <- 5

  #Initialize estimates for Omega_k, Omega_0
  #Omega_k - tune lambda by bic and then grab G and Omega_0
  lambda_grid <- 10^seq(-3, 0, length.out = 10)
  bic         <- vector(mode = "numeric", length = 0)

  #Tune lambda for independent glasso
  for (lam in lambda_grid) {
    omega_k <- ind_graphs(y, 0.1)
    bic   <- c(bic, bic_cal(y, omega_k))
  }
  #Compute initial est. via best bic
  omega_k <- ind_graphs(y, lambda_grid[which.min(bic)])
  
  #Loop through just to make sure PD
  for (k in 1:K) {
    if (any(eigen(omega_k[[k]])$values < 0)) {
      #print(k)
      omega_k[[k]] <- 
        omega_k[[k]] |>
        (\(x) {as.matrix(Matrix::nearPD(x)$mat)})()
    }
  }
  
  adj_k   <- map(.x = omega_k, ~abs(.x) > 0.001)
  #Omega0  <- apply(abind::abind(omega_k, along=3),1:2,mean)
  omega_0 <- Reduce("+", omega_k) / K

  #Initialize estimates for tau_k
  #Tau vector of subject specific regularization param on Omega_0
  tau_vec <- vector(mode = "numeric", length = K)

  #Iterate over each subject, find optimal tau_k based on posterior
  for (k in 1:K) {
    #print(k)
    #Tau posterior for fixed omega_k, omega_0, and lambda_2 = 0
    f_opt <- function(tau) {
      -1 * log_tau_posterior(tau, omega_k[[k]], omega_0, lambda_2 = 0)
    }
    #Optimize in 1D
    tau_vec[k] <- c(optimize(f_opt, interval = c(0, 1000), tol = 10)$min)
  }

  #Set up storage for results
  #Omegas
  omegas_res <- array(NA, c(p * (p + 1) / 2, K, n_samples))
  omega0_res <- array(NA, c(p * (p + 1) / 2, n_samples))
  pct_omega_acc <- vector(mode = "integer", length = n_samples)
  pct_k_acc  <- matrix(NA, nrow = n_samples, ncol = K)

  #Taus
  accept_mat    <- matrix(NA, nrow = 0, ncol = K)
  step_tau_mat  <- step_tau #Adaptive window for MH tau
  tau_res       <- array(NA, c(K, n_samples))

  #Lambdas
  lambda_res <- array(NA, c(3, n_samples), dimnames = list(str_c("lambda_", 1:3)))

  #Set timer
  timer   <- 0
  t_start <- proc.time()
  n_iter  <- (n_burn + n_samples)
  #n_iter  <- 3

  #Loop through sampling algorithm n_samples + n_burn # times
  for (t in 1:n_iter) {
    #Print iteration for early testing
    #print(paste0("Iteration: ", t))
    
    #Update Lambdas via direct sampling
    #Lambda 1 sparsity-inducing penalty on G_k
    card_k   <- (sapply(adj_k, sum) - p)/2 #Cardinality of G_k / # edges
    lambda_1 <- rgamma(1, alpha[1] + K, rate = beta[1] + sum(card_k))

    #Lambda 2 Exponential rate parameter for df/shrinkage tau_k prior
    lambda_2 <- rgamma(1, alpha[2] + K + 1, beta[2] + sum(tau_vec))

    #Lambda 3 Sparse L-1 penalty on group precision omega_0 prior
    card_0 <- (sum(abs(omega_0) > 0.001) + p) / 2 #Cardinality omega_0 / # Edges
    lambda_3 <- rgamma(1, alpha[3] + card_0, beta[3] + norm(omega_0, type = "1"))

    #Invert for Covariance & randomly select row_col pair
    sigma_0 <- matinv(omega_0)
    row_col <- sample(1:p, 1)
    
    #Set up foreach:: combine into 2 list, multicombine = TRUE
    # my_combine <- function(x, ...) {
    #   lapply(seq_along(x),
    #          function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
    # }
    
    #Update G_k, Omega_k via modified BIPS proposal & update scheme - Wang and Li (2012)
    omega_k <- omega_k

    #Tau_k update
    tau_k <-
      map(
        .x = 1:K, #Iterate from index 1 to K
        ~tau_update(tau_vec[.x], omega_k[[.x]], omega_0, lambda_2, step_tau[.x])
      ) #Return list object
    accept_mat <- rbind(accept_mat, tau_k %>% map_lgl("accept")) #Pull out acceptance
    tau_vec    <- tau_k %>% map_dbl("tau_k") #Pull out the numeric tau_k list object
    
    #Adaptive tau window/stepsize ~ variance/sigma in log normal
     if (t %% n_updates == 0 & t <= n_burn) {
        #Compute acceptance rate (colwise mean)
        accept_rate <- apply(accept_mat, 2, mean)
        #For each subject, adjust tau_k proposal (lognormal) step size
         for (k in 1:K) {
           if (accept_rate[k] > 0.75) { #If accepting to many, inc variance of proposal
            step_tau[k] <- step_tau[k] + 0.05
           } else if (accept_rate[k] < 0.5) { #If not accepting enough, dec variance of proposal
             step_tau[k] <- max(0.05, step_tau[k] - 0.05)
            }
         }
         step_tau_mat  <- rbind(step_tau_mat, step_tau) #Record adaptive step sizes
         accept_mat    <- matrix(NA, nrow = 0, ncol = K) #Restart acceptance rate tracking
     }

    #Update Omega_0 via Wang and Li (2012) + step-proposal distribution
    D       <- apply(mapply('/', omega_k, tau_vec, SIMPLIFY = 'array'), 1:2, sum)
    omega_0 <- omega0_update(omega_0, D, sum(tau_vec), lambda_3)
    pct_accept <- omega_0$pct_accept #Off-diagonal acceptance%
    omega_0 <- omega_0$omega #Precision matrix itself

    #Save those results after burn-in
    if(t > n_burn) {
      t_burn <- t - n_burn
      #omegas_res[, , t_burn] <- sapply(omega_k, function(x) x[upper.tri(x, diag = TRUE)])
      omega0_res[, t_burn]   <- omega_0[upper.tri(omega_0, diag = TRUE)]
      tau_res[, t_burn]      <- tau_vec
      #lambda_res[, t_burn]   <- c(lambda_1, lambda_2, lambda_3)
      pct_omega_acc[t_burn]  <- pct_accept
      #pct_k_acc[t_burn, ]    <- accept_k
    }

    #Track temporal progress (every 20% progress update)
    if (t %% floor(0.2 * (n_samples + n_burn))) {
      t_now   <- proc.time()
      timer   <- c(timer, (t_now - t_start)[3])
      t_start <- t_now
    }
  }

  #Result list of results
  omega0_result <-
    list(
      omega_0   = omega0_res,
      tau_k     = tau_res,
      tau_acc   = accept_mat,
      omega_acc = pct_omega_acc,
      timer     = timer
    )
  
  #Write out for safekeeping
  write_rds(omega0_result, "../results/prelim_omega0_tauk.RDS")
```

```{r omega0_tauk_diag, echo = FALSE}
#Read in result to display diagnostics
result <- read_rds("../results/prelim_omega0_tauk.RDS")
n_iter <- ncol(result$omega_0)
set.seed(4)
samp  <- sample(1:nrow(result$omega_0), 25, replace = FALSE)

#Diagnostics
chain0.gg <- 
  result$omega_0 %>%
  as.data.frame() %>%
  slice(samp) %>%
  rename_with(
    .cols = everything(),
    ~str_remove(.x, "V")
  ) %>%
  mutate(
    upper_tri = 1:nrow(.)
  ) %>%
  pivot_longer(
    cols = -upper_tri,
    names_to = "iteration",
    values_to = "value"
  ) %>%
  ggplot(aes(x = as.numeric(iteration), y = value, colour = upper_tri, group = upper_tri)) +
  geom_point(size = 0.8, alpha = 0.6) +
  geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~upper_tri, scales = "free_y", nrow = 5) +
  scale_x_continuous(breaks = seq(0, n_iter, by = floor(1/4*n_iter)), 
                     minor_breaks = seq(0, n_iter, by = floor(1/4*n_iter))) +
  scale_y_continuous(labels = scales::scientific) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_colour_viridis_c() +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Omega_0 Chain with Fixed G_k/Omega_k and Free Tau_k"
  )

#Acceptance
accept0.gg <- 
  tibble(
    x = result$omega_acc
  ) %>%
  ggplot(aes(x = x)) +
  geom_density(colour = "black", fill = my_purple, alpha = 0.2) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = "Percent Off-Diagonal Acceptance (MH-step)",
    y = "Density",
    title = "Omega_0 Off-diagonal Acceptance Rate"
  )

#Tauk
tauk.gg <-
  t(result$tau_k) %>%
  as.data.frame() %>%
  mutate(
    iteration = 1:nrow(.)
  ) %>%
  rename_with(
    .cols = -iteration,
    ~str_replace(.x, "V", "Sub. ")
  ) %>%
  pivot_longer(
    cols = -iteration,
    names_to = "tau",
    values_to = "value"
  ) %>%
  mutate(
    tau = as.factor(tau) %>% fct_reorder(value, mean, .desc = TRUE)
  ) %>%
  ggplot(aes(x = iteration, y = value, colour = tau)) +
  geom_point(size = 0.8, alpha = 0.6) +
  geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~tau, scales = "free_y", nrow = 5) +
  scale_x_continuous(breaks = seq(0, n_iter, by = floor(1/4*n_iter)), 
                     minor_breaks = seq(0, n_iter, by = floor(1/4*n_iter))) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Tau_k Chain with Fixed G_k/Omega_k, Varied Omega_0"
  )

#Tau_k acceptance
tauk_acc.gg <-
result$tau_acc %>%
  as.data.frame() %>%
  mutate(
    iteration = 1:nrow(.)
  ) %>%
  rename_with(
    .cols = -iteration,
    ~str_replace(.x, "V", "Sub. ")
  ) %>%
  pivot_longer(
    cols = -iteration,
    names_to = "tau",
    values_to = "value"
  ) %>%
  group_by(tau) %>%
  summarise(pct_acc = mean(value)) %>%
  ungroup() %>%
  mutate(
    tau = as.factor(tau) %>% fct_reorder(pct_acc, .desc = FALSE)
  ) %>%
  arrange(tau) %>%
  ggplot(aes(x = tau, y = pct_acc, colour = pct_acc, fill = pct_acc)) +
  geom_col() +
  scale_colour_viridis_c("Pct. Accept", breaks = seq(0.9, 1, by = 0.05), labels = scales::percent, direction = -1) +
  scale_fill_viridis_c("Pct. Accept", breaks = seq(0.9, 1, by = 0.05), labels = scales::percent, direction = -1) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Subject",
    y = "Tau Acceptance",
    title = "Tau Acceptance for Fixed G_k/Omega_k but Varied Omega_0"
  ) +
  theme(legend.position = "left") +
  coord_flip() +
  theme_minimal()
```

```{r omega0_tauk_diag_display, echo = FALSE, fig.height = 6, fig.width = 6}
chain0.gg
accept0.gg
tauk.gg
tauk_acc.gg
```

### 2.2.3 $\tau_k$  

```{r tauk, eval = FALSE, echo = FALSE}
#Set params
y <- data_list
n_samples <- 100
n_burn <- 20
    
  #Grab no. of subjects, rois, volumes
  K  <- length(y)
  p  <- ncol(y[[1]])
  vk <- map_dbl(y, nrow)
  Sk <- map(.x = y, ~t(.x) %*% .x)

  #Set lambda 1-3 penalty gamma a, b  hyperparams
  alpha <- c(0.5, 1, 0.5) #1 - G_k, 2 - tau_k, 3 - Omega_0 (glasso)
  beta  <- c(0.5, 1, 0.5)

  #Set tau's MH stepsize
  step_tau  <- rep(1, K)
  #n_updates <- 5

  #Initialize estimates for Omega_k, Omega_0
  #Omega_k - tune lambda by bic and then grab G and Omega_0
  lambda_grid <- 10^seq(-3, 0, length.out = 10)
  bic         <- vector(mode = "numeric", length = 0)

  #Tune lambda for independent glasso
  for (lam in lambda_grid) {
    omega_k <- ind_graphs(y, 0.1)
    bic   <- c(bic, bic_cal(y, omega_k))
  }
  #Compute initial est. via best bic
  omega_k <- ind_graphs(y, lambda_grid[which.min(bic)])
  
  #Loop through just to make sure PD
  for (k in 1:K) {
    if (any(eigen(omega_k[[k]])$values < 0)) {
      #print(k)
      omega_k[[k]] <- 
        omega_k[[k]] |>
        (\(x) {as.matrix(Matrix::nearPD(x)$mat)})()
    }
  }
  
  adj_k   <- map(.x = omega_k, ~abs(.x) > 0.001)
  #Omega0  <- apply(abind::abind(omega_k, along=3),1:2,mean)
  omega_0 <- Reduce("+", omega_k) / K

  #Initialize estimates for tau_k
  #Tau vector of subject specific regularization param on Omega_0
  tau_vec <- vector(mode = "numeric", length = K)

  #Iterate over each subject, find optimal tau_k based on posterior
  for (k in 1:K) {
    #print(k)
    #Tau posterior for fixed omega_k, omega_0, and lambda_2 = 0
    f_opt <- function(tau) {
      -1 * log_tau_posterior(tau, omega_k[[k]], omega_0, lambda_2 = 0)
    }
    #Optimize in 1D
    tau_vec[k] <- c(optimize(f_opt, interval = c(0, 1000), tol = 10)$min)
  }

  #Set up storage for results
  #Omegas
  omegas_res <- array(NA, c(p * (p + 1) / 2, K, n_samples))
  omega0_res <- array(NA, c(p * (p + 1) / 2, n_samples))
  pct_omega_acc <- vector(mode = "integer", length = n_samples)
  pct_k_acc  <- matrix(NA, nrow = n_samples, ncol = K)

  #Taus
  accept_mat    <- matrix(NA, nrow = 0, ncol = K)
  step_tau_mat  <- step_tau #Adaptive window for MH tau
  tau_res       <- array(NA, c(K, n_samples))

  #Lambdas
  lambda_res <- array(NA, c(3, n_samples), dimnames = list(str_c("lambda_", 1:3)))

  #Set timer
  timer   <- 0
  t_start <- proc.time()
  n_iter  <- (n_burn + n_samples)
  #n_iter  <- 3

  #Loop through sampling algorithm n_samples + n_burn # times
  for (t in 1:n_iter) {
    #Print iteration for early testing
    #print(paste0("Iteration: ", t))
    
    #Update Lambdas via direct sampling
    #Lambda 1 sparsity-inducing penalty on G_k
    card_k   <- (sapply(adj_k, sum) - p)/2 #Cardinality of G_k / # edges
    lambda_1 <- rgamma(1, alpha[1] + K, rate = beta[1] + sum(card_k))

    #Lambda 2 Exponential rate parameter for df/shrinkage tau_k prior
    lambda_2 <- rgamma(1, alpha[2] + K + 1, beta[2] + sum(tau_vec))

    #Lambda 3 Sparse L-1 penalty on group precision omega_0 prior
    card_0 <- (sum(abs(omega_0) > 0.001) + p) / 2 #Cardinality omega_0 / # Edges
    lambda_3 <- rgamma(1, alpha[3] + card_0, beta[3] + norm(omega_0, type = "1"))

    #Invert for Covariance & randomly select row_col pair
    sigma_0 <- matinv(omega_0)
    row_col <- sample(1:p, 1)
    
    #Set up foreach:: combine into 2 list, multicombine = TRUE
    # my_combine <- function(x, ...) {
    #   lapply(seq_along(x),
    #          function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
    # }
    
    #Update G_k, Omega_k via modified BIPS proposal & update scheme - Wang and Li (2012)
    omega_k <- omega_k

    #Tau_k update
    tau_k <-
      map(
        .x = 1:K, #Iterate from index 1 to K
        ~tau_update(tau_vec[.x], omega_k[[.x]], omega_0, lambda_2, step_tau[.x])
      ) #Return list object
    accept_mat <- rbind(accept_mat, tau_k %>% map_lgl("accept")) #Pull out acceptance
    tau_vec    <- tau_k %>% map_dbl("tau_k") #Pull out the numeric tau_k list object
    
    #Adaptive tau window/stepsize ~ variance/sigma in log normal
     if (t %% n_updates == 0 & t <= n_burn) {
        #Compute acceptance rate (colwise mean)
        accept_rate <- apply(accept_mat, 2, mean)
        #For each subject, adjust tau_k proposal (lognormal) step size
         for (k in 1:K) {
           if (accept_rate[k] > 0.75) { #If accepting to many, inc variance of proposal
            step_tau[k] <- step_tau[k] + 0.05
           } else if (accept_rate[k] < 0.5) { #If not accepting enough, dec variance of proposal
             step_tau[k] <- max(0.05, step_tau[k] - 0.05)
            }
         }
         step_tau_mat  <- rbind(step_tau_mat, step_tau) #Record adaptive step sizes
         accept_mat    <- matrix(NA, nrow = 0, ncol = K) #Restart acceptance rate tracking
     }

    #Update Omega_0 via Wang and Li (2012) + step-proposal distribution
    D       <- apply(mapply('/', omega_k, tau_vec, SIMPLIFY = 'array'), 1:2, sum)
    omega_0 <- omega_0

    #Save those results after burn-in
    if(t > n_burn) {
      t_burn <- t - n_burn
      #omegas_res[, , t_burn] <- sapply(omega_k, function(x) x[upper.tri(x, diag = TRUE)])
      #omega0_res[, t_burn]   <- omega_0[upper.tri(omega_0, diag = TRUE)]
      tau_res[, t_burn]      <- tau_vec
      #lambda_res[, t_burn]   <- c(lambda_1, lambda_2, lambda_3)
      #pct_omega_acc[t_burn]  <- pct_accept
      #pct_k_acc[t_burn, ]    <- accept_k
    }

    #Track temporal progress (every 20% progress update)
    if (t %% floor(0.2 * (n_samples + n_burn))) {
      t_now   <- proc.time()
      timer   <- c(timer, (t_now - t_start)[3])
      t_start <- t_now
    }
  }

  #Result list of results
  omega0_result <-
    list(
      tau_k     = tau_res,
      tau_acc   = accept_mat,
      tau_step  = step_tau_mat
    )
  
  #Write out for safekeeping
  write_rds(omega0_result, "../results/prelim_tauk.RDS")
```

```{r tauk_diag, echo = FALSE}
#Read in result to display diagnostics
result <- read_rds("../results/prelim_tauk.RDS")
n_iter <- ncol(result$tau_k)
set.seed(4)

#Diagnostics
#Tauk
tauk.gg <-
  t(result$tau_k) %>%
  as.data.frame() %>%
  mutate(
    iteration = 1:nrow(.)
  ) %>%
  rename_with(
    .cols = -iteration,
    ~str_replace(.x, "V", "Sub. ")
  ) %>%
  pivot_longer(
    cols = -iteration,
    names_to = "tau",
    values_to = "value"
  ) %>%
  mutate(
    tau = as.factor(tau) %>% fct_reorder(value, mean, .desc = TRUE)
  ) %>%
  ggplot(aes(x = iteration, y = value, colour = tau)) +
  geom_point(size = 0.8, alpha = 0.6) +
  geom_line(size = 0.6, alpha = 0.8) +
  facet_wrap(~tau, scales = "free_y", nrow = 5) +
  scale_x_continuous(breaks = seq(0, n_iter, by = floor(1/4*n_iter)), 
                     minor_breaks = seq(0, n_iter, by = floor(1/4*n_iter))) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_colour_viridis_d() +
  labs(
    x = "Iteration",
    y = "Value",
    title = "Tau_k Chain with Fixed G_k/Omega_k, Varied Omega_0"
  )

#Tau_k acceptance
tauk_acc.gg <-
result$tau_acc %>%
  as.data.frame() %>%
  mutate(
    iteration = 1:nrow(.)
  ) %>%
  rename_with(
    .cols = -iteration,
    ~str_replace(.x, "V", "Sub. ")
  ) %>%
  pivot_longer(
    cols = -iteration,
    names_to = "tau",
    values_to = "value"
  ) %>%
  group_by(tau) %>%
  summarise(pct_acc = mean(value)) %>%
  ungroup() %>%
  mutate(
    tau = as.factor(tau) %>% fct_reorder(pct_acc, .desc = FALSE)
  ) %>%
  arrange(tau) %>%
  ggplot(aes(x = tau, y = pct_acc, colour = pct_acc, fill = pct_acc)) +
  geom_col() +
  scale_colour_viridis_c("Pct. Accept", breaks = seq(0.9, 1, by = 0.05), labels = scales::percent, direction = -1) +
  scale_fill_viridis_c("Pct. Accept", breaks = seq(0.9, 1, by = 0.05), labels = scales::percent, direction = -1) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Subject",
    y = "Tau Acceptance",
    title = "Tau Acceptance for Fixed G_k/Omega_k but Varied Omega_0"
  ) +
  theme(legend.position = "left") +
  coord_flip() +
  theme_minimal()
```

```{r tauk_diag_display, echo = FALSE, fig.height=6, fig.width=6}
tauk.gg
tauk_acc.gg
result$tau_step %>%
  as.data.frame() %>%
  mutate(tau_step = paste0("Adaptive step ", 1:nrow(.))) %>%
  rename_with(
    .cols = everything(),
    ~str_replace(.x, "V", "Subj. ")
  ) %>%
  dplyr::select(tau_step, everything()) %>%
  group_by(tau_step) %>%
  gt() %>%
  tab_header(title = "Adaptive Tau-proposal Step-size/Variance")
```


### 2.2.2 $\Omega_k$ & $G_k$  



### 2.2.4 Sensitivity to Regularization Hyperparameters $(\lambda_j \, | \, a_j, b_j \sim \Gamma(a_j, b_j))$  

### 2.2.1 Posterior Distribution  

### 2.2.2. Accept/Reject Rates for Proposals  
